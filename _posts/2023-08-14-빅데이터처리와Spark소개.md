---
title : 빅데이터 처리와 소개 
date : 2023-08-14 23:00 +09:00
categories : [Data, spark]
tags : [프로그래머스, 파이썬으로 해보는 spark 프로그래밍 with 프로그래머스, DE, Spark]

---

[1. 빅데이터의 정의와 예](#1-빅데이터의-정의와-예)
<br>
[2. 빅데이터 처리가 갖는 특징](#2-빅데이터-처리가-갖는-특징)
<br>
[3. 하둡의 등장과 소개](#3-하둡의-등장과-소개)
<br>
[4. 맵리듀스 프로그래밍 소개](#4-맵리듀스-프로그래밍-소개)
<br>
[5. 하둡 설치](#5-하둡-설치)
<br>
[6. Spark 소개](#6-spark-소개)
<br>
[7. Spark 프로그램 실행 옵션](#7-spark-프로그램-실행-옵션)
<br>


# 1. 빅데이터의 정의와 예

## 1. 빅데이터 정의(1)

- “서버 한대로 처리할 수 없는 규모의 데이터”
- 2012년 4월 아마존 클라우드 컨퍼런스에서 아마존의 data scientist인 존 라우저(John Rauser)가 내린 정의 **분산 환경**이 필요하느냐에 포커스

- 판다스로 처리해야 할 데이터가 너무 커서 처리가 불가능하다면 어떻게 할 것인가?

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/e4a06be8-6d8a-422f-93e4-8ed040cb887d)

## 2. 빅데이터 정의(2)

- “기존의 소프트웨어로는 처리할 수 없는 규모의 데이터”
- 대표적인 기존 소프트웨어 오라클이나 MySQL과 같은 **관계형 데이터베이스**
    - 분산환경을 염두에 두지 않음
    - Scale-up 접근방식 (vs. Scale-out)
        - 메모리 추가, CPU 추가, 디스크 추가

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/b009fc78-ef93-4810-b6a3-13e6e64917e6)

    
## 3. 빅데이터 정의(3)

- 4V (Volume, Velocity, Variety, Varecity)
- **Volume**: 데이터의 크기가 대용량?
- **Velocity**: 데이터의 처리 속도가 중요?
- **Variety**: 구조화/비구조화 데이터 둘다?
- **Veracity**: 데이터의 품질이 좋은지?

## 4. 빅데이터의 예

### 1. 디바이스 데이터

- 모바일 디바이스
    - 위치정보
- 스마트 TV
- 각종 센서 데이터 (IoT 센서)
- 네트워킹 디바이스

### 2. 웹

- 수십 조개 이상의 웹 페이지 존재 -> 온갖 종류의 지식의 바다(정보가 너무 많음)
- 웹 검색엔진 개발은 진정한 대용량 데이터 처리
    - 웹 페이지를 크롤하여 중요한 페이지를 찾아내고 (페이지 랭크)
    인덱싱하고 서빙
    - 구글이 빅데이터 기술의 발전에 지대한 공헌
- 사용자 검색어와 클릭 정보 자체도 대용량
    - 이를 마이닝하여 개인화 혹은 별도 서비스 개발이 가능
        - 검색어를 바탕으로한 트렌드 파악, 통계 기반 번역, …
- 요즘은 웹 자체가 **NLP(Natural Language Processing, 자연어 처리)**  거대 모델 개발의 훈련 데이터로 사용되고 있음

# 2. 빅데이터 처리가 갖는 특징

## 1. 빅데이터 처리의 특징

### 1. 문제점

- 먼저 큰 데이터를 **손실없이** 보관할 방법이 필요: **스토리지**
- 처리 시간이 오래 걸림: **병렬처리**
- 이런 데이터들은 **비구조화**된 데이터일 가능성이 높음: SQL만으로는 부족 → ex) 웹 로그 파일

### 2. 해결방안

- 큰 데이터를 손실없이 보관할 방법이 필요
    - 큰 데이터 저장이 가능한 **분산 파일 시스템**이 필요
- 시간이 오래 걸림
    - 병렬 처리가 가능한 **분산 컴퓨팅 시스템**이 필요
- 이런 데이터들은 비구조화된 데이터일 가능성이 높음
    - 비구조화 데이터를 처리할 방법이 필요
    결국

### 3. 대용량 분산 시스템

- 분산 환경 기반 (1대 혹은 그 이상의 서버로 구성)
    - 분산 파일 시스템과 분산 컴퓨팅 시스템이 필요
- Fault Tolerance
    - 하나 이상의 구성 요소가 고장나도 시스템이 중단 없이 계속 작동할 수 있는 능력
    - 소수의 서버가 고장나도 동작해야함
- 확장이 용이해야함
    - Scale Out이 되어야함
    

# 3. 하둡의 등장과 소개

## 1. 하둡(Hadoop)의 등장

- Doug Cutting이 구글랩 발표 논문들에 기반해 만든 오픈소스
프로젝트
    - 2003년 The Google File System
    - 2004년 MapReduce: Simplified Data Processing on Large Cluster
- 처음 시작은 Nutch라는 오픈소스 검색엔진의 하부 프로젝트
    - 하둡은 Doug Cutting의 아들의 코끼리 인형의 이름
    - 2006년에 아파치 톱레벨 별개 프로젝트로 떨어져나옴

## 2. 하둡(Hadoop)이란?

- Hortonworks(회사 이름)의 정의
    - HDP(Hortonworks Data planform)
    - An open source software platform for distributed storage(분산 파일 시스템인 HDFS) and distributed processing(분산 컴퓨팅 MapReduce) of very large data sets on computer clusters built from commodity hardware
    - 분산 스토리지 및 대규모 멀티 소스 데이터 세트 처리가 가능한 오픈 소스 프레임워크
    

## 3. 하둡(Hadoop)의 발전

- 하둡 1.0은 HDFS위에 MapReduce라는 분산컴퓨팅 시스템이 도는 구조
    - MapReduce: 대용량의 데이터를 분산/병렬 컴퓨팅 환경에서 처리하기 위해 제작된 데이터 처리 모델
- MapReduce 위에서 다양한 컴퓨팅 언어들이 만들어짐
    - PIG, Hive, presto

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/51e7a091-ecdd-4f13-b82a-264c4512f746)

- 하둡 2.0에서 아키텍처가 크게 변경됨
    - 하둡은 YARN이란 이름의 분산처리 시스템위에서 동작하는
    애플리케이션이 됨
- Spark은 YARN위에서 애플리케이션 레이어로 실행됨

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/db9f7f95-81f5-4b63-9e69-0f171d9b9b50)

## 4. HDFS - 분산 파일 시스템

- 데이터를 블록단위로 나눠 저장
    - 블록의 크기는 128 MB (디폴트)
- 블록 복제 방식 (Replication)
    - 각 블록은 3 군데에 중복 저장됨
    - Fault tolerance를 보장할 수 있는 방식으로 이 블록들은 저장됨
- 하둡 2.0 네임노드 이중화 지원
    - Active & Standby
        - 둘 사이에 share edit log가 존재
    - Secondary 네임노드는 여전히 존재

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/37dd4881-5909-43c0-a5b4-b1d6ff5350ab)

1. **NameNode(네임 노드- master )**
- 파일시스템 이름공간을 관리하고 클라이언트의 파일 접근(열기,닫기,이름 바꾸기 등)을 조정한다.
- 입력 데이터는 불록들로 나뉘며 어느 블록이 어떤 데이터 노드에 저장될 것인지를 알려준다.

2. **DataNode(데이터 노드 -slave)**
- 분할된 데이터셋 복제본을 저장하고 요청에 대해 데이터를 제공해주는 슬레이브 컴퓨터다.
- 또한 블록 생성과 삭제도 수행한다.

## 5. MapReduce - 분산 컴퓨팅 시스템

- 하둡 1.0
- 하나의 잡 트래커와 다수의 태스크 트래커로 구성됨
    - 잡 트래커가 일을 나눠서 다수의 태스크 트래커에게 분배
    - 태스크 트래커에서 병렬처리
- MapReduce만 지원
    - 제너럴한 시스템이 아님
![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/8ecc05c8-bcab-411c-96ac-ce6215fdd10e)


## 6. YARN

### 1. 분산 컴퓨팅 시스템: 하둡 2.0 (YARN 1.0)

- 세부 리소스 관리가 가능한 범용 컴퓨팅 프레임웍
    - **Resource Manager(리소스 매니저)**
        - 클러스터 전체를 관리하는 master 서버의 역할
        - `Job Scheduler`, `Application Manager`로 나눠서 일함
    - **Node Manager(노드 매니저)**
        - 노드당 하나씩 존재하며, 슬레이브 노드(slave node)의 자원을 모니터링(monitoring) 하고 관리하는 역할
        - 리소스 매니저의 지시를 받아 작업 요구사항에 따라서 컨테이너를 생성
        - **`Container`** : ****노드 매니저가 실행되는 서버의 시스템 자원
        - **`Application Master`** : 노드 매니저가 실행되는 서버의 시스템 자원
    - **컨테이너**
        - 앱 마스터
        - 태스크
- Spark이 이 YARN위에서 구현됨

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/e5143e17-d310-4af1-88eb-813d54b57a78)


### 2. YARN의 동작
![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/54d9de2f-c5cd-4856-9010-7ab69c3b0ead)

**YARN의 상세 과정**

1. 실행하려는 코드와 환경 정보를 RM(Resource Manager)에게 넘김
    - 실행에 필요한 파일들은 application ID에 해당하는 HDFS 폴더에 미리 복사됨
2. RM은 NM(Node Manager)으로부터 컨테이너를 받아 AM(Application Master) 실행
    - AM은 프로그램 마다 하나씩 할당되는 프로그램 마스터에 해당
3. AM은 입력 데이터 처리에 필요한 리소스를 RM에게 요구
    - RM은 data locality를 고려해서 리소스(컨테이너)를 할당
4. AM은 할당받은 리소스를 NM을 통해 컨테이너로 론치하고 그 안에서 코드를 실행
    - 이 때 실행에 필요한 파일들이 HDFS에서 Container가 있는 서버로 먼저 복사
5. 각 태스크는 상황을 주기적으로 AM에게 보고 (heartbeat)
    - 태스크가 실패하거나 보고가 오랜 시간 없으면 태스크를 다른 컨테이너로 재실행

## 7. 하둡 1.0 vs 하둡2.0

- 하둡 2.0에서 소개된 클러스터 자원 관리자를 **YARN**이라고 부름

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/f6903ec7-ffb6-4bc3-8d28-543c5f5736d0)

- Spark 은 YARN 위의 Other Application에 속함

## 8.  하둡 3.0의 특징(YARN 2.0)

- **YARN 2.0**
    - YARN 프로그램들의 논리적인 그룹(플로우라고 부름)으로 나눠서 자원 관리가 가능. 이를 통해 데이터 수집 프로세스와 데이터 서빙 프로세스를나눠서 관리 가능
    - 타임라인 서버에서 HBase를 기본 스토리지로 사용 (하둡 2.1)
- **파일 시스템**
    - 네임노드의 경우 다수의 스탠바이 임노드를 지원
    - HDFS, S3, Azure Storage 이외에도 Azure Data Lake Storage 등을 지원

# 4. 맵리듀스 프로그래밍 소개

## 1. 맵리듀스 프로그래밍의 특징

- 데이터 셋은 Key, Value의 집합이며 변경 불가(immutable)
- 데이터 조작은 map과 reduce 두 개의 오퍼레이션으로만 가능
    - 이 두 오퍼레이션은 항상 하나의 쌍으로 연속으로 실행됨
    - 이 두 오퍼레이션의 코드를 개발자가 채워야함
- 맵리듀스 시스템이 Map의 결과를 Reduce단으로 모아줌
    - 이 단계를 보통 **셔플링**이라 부르며 **네트워크 단**을 통한 데이터 이동이 생김

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/bd348754-c22e-46a8-a309-7c5f7e5ec391)


## 2. 맵리듀스 프로그래밍의 핵심: Map과 Reduce

### 1. Map: (k, v) -> [(k', v')*]

- 입력은 시스템에 의해 주어지며 입력으로 지정된 HDFS 파일에서 넘어옴
- 키,밸류 페어를 새로운 키,밸류 페어 리스트로 변환 (transformation
- 출력: 입력과 동일한 키, 밸류 페어를 그대로 출력해도 되고 출력이 없어도 됨

### 2. Reduce: (k’, [v1’, v2’, v3’, v4’, …]) -> (k’’, v'')

- 입력은 시스템에 의해 주어짐
- 맵의 출력 중 같은 키를 갖는 키/밸류 페어를 시스템이 묶어서 입력으로 넣어줌
- 키와 밸류 리스트를 새로운 키,밸류 페어로 변환
- SQL의 GROUP BY와 흡사
- 출력이 HDFS에 저장됨

## 3. MapReduce 프로그램 동작 예시

- Word Count

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/7d49b3c3-e990-4ae6-b5c7-626f0911e848)


## 4. MapReduce: 프로그래밍 예제

### 1. WordCount Mapper

https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java

```java
public class WordCount {

  public static class TokenizerMapper 
       extends Mapper<Object, Text, Text, IntWritable>{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
```

```java
Map: (k, v) -> [(k', v')*]
```

- Transformation
- 키,밸류 페어 → 새로운 키,밸류 페어 리스트로 변환

```java
Input: (100, “the brave yellow lion”)
Output:
[
(“the”, 1),
(“brave”, 1),
(“yellow”, 1),
(“lion”, 1)
]
```

### 2. Word Count Reducer

```java
public static class IntSumReducer 
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
```

```java
Reduce: (k’, [v1’, v2’, v3’, v4’, …]) -> (k’’, v'')
```

- SQL의 GROUP BY와 동일
- 키,밸류 리스트 → 새로운 키,밸류 페어로 변환

```java
Input: ("lion": [1, 1, 1])
Output: ("lion": 3)
```

## 5. MapReduce: Shuffling and Sorting

### 1. Shuffling

- Mapper의 출력을 Reducer로 보내주는 프로세스를 말함
- 전송되는 데이터의 크기가 크면 네트워크 병목을 초래하고 시간이 오래 걸림

### 2. Sorting

- 모든 Mapper의 출력을 Reducer가 받으면 이를 키별로 sorting

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/0c72d33e-4298-4d3e-9d1d-a03d55720650)


### 3. 전체 과정
![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/88edb030-7bb1-4aeb-b1db-9a05ae8ab266)


## 6. MapReduce: Data Skew

- IF 각 task 처리하는 데이터 크기에 불균형이 존재한다면?

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/095fdcef-f015-4f1d-a65f-224c046d37f3)

- 병렬처리의 큰 의미가 없음 → 가장 느린 태스크가 전체 처리 속도를 결정
- 특히 Reducer로 오는 데이터 크기는 큰 차이가 있을 수 있음
    - Group By나 Join등에 이에 해당함
    - 처리 방식에 따라 Reducer의 수에 따라 메모리 에러 등이 날 수 있음
- 데이터 엔지니어가 고생하는 이유 중의 하나
    - 빅데이터 시스템에는 이 문제가 모두 존재

## 7. MapReduce 프로그래밍의 문제점

### 1. 낮은 생산성

- 프로그래밍 모델이 가진 융통성 부족 (Map과 Reduce2가지 오퍼레이션만 지원)
- 튜닝/최적화가 쉽지 않음
    - 예) 데이터 분포가 균등하지 않은 경우
- 생산성이 떨어짐. 데이터 모델과 오퍼레이션에 제약이 많음
- 모든 입출력이 **디스크**를 통해 이뤄짐
    - 큰 데이터 배치 프로세싱에 적합

### 2. 배치작업 중심

- 기본적으로 Low Latency가 아니라 **Throughput**에 초점이 맞춰짐

### 3. Shuffling 이후에 Data Skew가 발생하기 쉬움

- Reduce 태스크 수를 개발자가 지정해주어야함

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/336e7cc5-d489-4106-815d-73df61c22a7d)

## 8. MapReduce 대안들의 등장

- 더 범용적인 대용량 데이터 처리 프레임웍들의 등장
    - YARN, Spark
- SQL의 컴백: Hive, Presto등이 등장
    - Hive
        - MapReduce위에서 구현됨. Throughput에 초점. 대용량 ETL에 적합
- Presto
    - Low latency에서 초점. 메모리를 주로 사용. Adhoc 쿼리에 적합
    - AWS Athena가 Presto 기반

# 5. 하둡 설치

## 1. 하둡 설치

- 우분투에서 설치

 https://phoenixnap.com/kb/install-hadoop-ubuntu

## 2. WordCount 실행

### 1. input 파일 생성
![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/8e6891fa-5e4a-4da0-8347-8c387fd76fa6)

```bash
# root 설정
bin/hdfs dfs -mkdir /user 
bin/hdfs dfs -mkdir /user/hdoop

# input 파일 생성
bin/hdfs dfs -mkdir input
bin/hdfs dfs -ls
```

### 2. words.txt 생성
![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/60b41c39-4437-435b-975c-25e6506a88b1)

- 랜덤 단어를 words.txt에 작성
- `-put` 으로 input파일에 words.txt 배치

### 3. WordCount 실행

1. **wordcount 실행**

```bash
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount input output
```

2. **결과**

<details>
<summary>실행 결과</summary>
<div markdown="1">
    ```bash
    2023-08-12 17:38:41,450 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /127.0.0.1:8032
    2023-08-12 17:38:41,617 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdoop/.staging/job_1691828877731_0001
    2023-08-12 17:38:41,738 INFO input.FileInputFormat: Total input files to process : 1
    2023-08-12 17:38:41,770 INFO mapreduce.JobSubmitter: number of splits:1
    2023-08-12 17:38:41,850 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1691828877731_00012023-08-12 17:38:41,850 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2023-08-12 17:38:41,921 INFO conf.Configuration: resource-types.xml not found
    2023-08-12 17:38:41,921 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2023-08-12 17:38:42,013 INFO impl.YarnClientImpl: Submitted application application_1691828877731_00012023-08-12 17:38:42,029 INFO mapreduce.Job: The url to track the job: http://DESKTOP-JK8PE1R.localdomain:8088/proxy/application_1691828877731_0001/
    2023-08-12 17:38:42,030 INFO mapreduce.Job: Running job: job_1691828877731_0001
    2023-08-12 17:38:47,070 INFO mapreduce.Job: Job job_1691828877731_0001 running in uber mode : false
    2023-08-12 17:38:47,071 INFO mapreduce.Job:  map 0% reduce 0%
    2023-08-12 17:38:50,094 INFO mapreduce.Job:  map 100% reduce 0%
    2023-08-12 17:38:54,111 INFO mapreduce.Job:  map 100% reduce 100%
    2023-08-12 17:38:55,126 INFO mapreduce.Job: Job job_1691828877731_0001 completed successfully
    2023-08-12 17:38:55,163 INFO mapreduce.Job: Counters: 54
            File System Counters
                    FILE: Number of bytes read=103
                    FILE: Number of bytes written=552093
                    FILE: Number of read operations=0
                    FILE: Number of large read operations=0
                    FILE: Number of write operations=0
                    HDFS: Number of bytes read=178
                    HDFS: Number of bytes written=61
                    HDFS: Number of read operations=8
                    HDFS: Number of large read operations=0
                    HDFS: Number of write operations=2
                    HDFS: Number of bytes read erasure-coded=0
            Job Counters
                    Launched map tasks=1
                    Launched reduce tasks=1
                    Data-local map tasks=1
                    Total time spent by all maps in occupied slots (ms)=1146
                    Total time spent by all reduces in occupied slots (ms)=1154
                    Total time spent by all map tasks (ms)=1146
                    Total time spent by all reduce tasks (ms)=1154
                    Total vcore-milliseconds taken by all map tasks=1146
                    Total vcore-milliseconds taken by all reduce tasks=1154
                    Total megabyte-milliseconds taken by all map tasks=1173504
                    Total megabyte-milliseconds taken by all reduce tasks=1181696
            Map-Reduce Framework
                    Map input records=1
                    Map output records=14
                    Map output bytes=121
                    Map output materialized bytes=103
                    Input split bytes=113
                    Combine input records=14
                    Combine output records=9
                    Reduce input groups=9
                    Reduce shuffle bytes=103
                    Reduce input records=9
                    Reduce output records=9
                    Spilled Records=18
                    Shuffled Maps =1
                    Failed Shuffles=0
                    Merged Map outputs=1
                    GC time elapsed (ms)=13
                    CPU time spent (ms)=470
                    Physical memory (bytes) snapshot=556724224
                    Virtual memory (bytes) snapshot=5534662656
                    Total committed heap usage (bytes)=528482304
                    Peak Map Physical memory (bytes)=329510912
                    Peak Map Virtual memory (bytes)=2765922304
                    Peak Reduce Physical memory (bytes)=227213312
                    Peak Reduce Virtual memory (bytes)=2768740352
            Shuffle Errors
                    BAD_ID=0
                    CONNECTION=0
                    IO_ERROR=0
                    WRONG_LENGTH=0
                    WRONG_MAP=0
                    WRONG_REDUCE=0
            File Input Format Counters
                    Bytes Read=65
            File Output Format Counters
                    Bytes Written=61
    ```
</div>
</details>

3. **Output 확인**

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/ff1eed43-ee1d-44e9-b8eb-801c13a951d0)


- output/_SUCCESS : 실행 결과 status
- output/part-r-00000 : count 결과 값

1. **Output 출력**

```bash
bin/hdfs dfs -cat output/part-r-00000
```

# 6. Spark 소개

## 1. Spark의 등장

- 버클리 대학의 AMPLab에서 아파치 오픈소스 프로젝트로 2013년 시작
    - 나중에 Databricks라는 스타트업 창업
- 하둡의 뒤를 잇는 2세대 빅데이터 기술
    - YARN등을 분산환경으로 사용
    - Scala로 작성됨
- 빅데이터 처리 관련 *다양한* 기능 제공

## 2. Spark 3.0의 구성

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/b328b10b-089c-4c66-987a-5e6446f95365)


- Spark Core
- Spark SQL
- Spark ML
    - Spark MLlib
- Spark Streaming
- Spark GraphX

## 3. Spark vs MapReduce

- Spark은 기본적으로 **메모리 기반**
    - 메모리가 부족해지면 디스크 사용
    - MapReduce는 디스크 기반 → 상대적으로 느림, 실시간 분석 어려
- MapReduce는 **하둡(YARN)**위에서만 동작
    - Spark은 하둡(YARN)이외에도 다른 분산 컴퓨팅 환경 지원 (K8s, Mesos)
- MapReduce는 키와 밸류 기반 데이터 구조만 지원
    - Spark은 판다스 데이터프레임과 개념적으로 동일한 데이터 구조 지원
- Spark은 다양한 방식의 컴퓨팅을 지원
    - 배치 데이터 처리, 스트림 데이터 처리, SQL, 머신 러닝, 그래프 분석

## 4. Spark 프로그래밍 API

### 1. RDD (Resilient Distributed Dataset)

- **로우레벨** 프로그래밍 API로 **세밀한 제어**가 가능
- 하지만 **코딩 복잡도** 증가

### 2. DataFrame & Dataset (판다스의 데이터프레임과 흡사)

- **하이레벨** 프로그래밍 API로 점점 많이 사용되는 추세
- 구조화 데이터 조작이라면 보통 **Spark SQL**을 사용
- DataFrame/Dataset이 꼭 필요한 경우는?
    - ML 피쳐 엔지니어링을 하거나 Spark ML을 쓰는 경우
    - SQL만으로 할 수 없는 일의 경우

## 5. Spark SQL

- Spark SQL은 구조화된 데이터 처리를 SQL로 처리
- 데이터 프레임을 SQL로 처리 가능
    - 데이터프레임은 테이블처럼 sql로 처리 가능
    - 판다스도 동일 기능 제공
- Hive 쿼리 보다 최대 100배까지 빠른 성능을 보장
    - 사실은 그렇지 않음. Hive도 그 사이에 메모리를 쓰는 걸로 발전
        - Hive: 디스크 -> 메모리
        - spark: 메모리 -> 디스크
        - Presto: 메모리 -> 디스크
        

## 6. Spark ML

- 머신러닝 관련 다양한 알고리즘, 유틸리티로 구성된 라이브러리
- Classification, Regression, Clustering, Collaborative Filtering, …
    - 전체 리스트 https://spark.apache.org/docs/latest/ml-classification-regression.html
    - 딥러닝 지원은 미약
- RDD 기반과 데이터프레임 기반의 두 버전이 존재
    - spark.mllib vs. [spark.ml](http://spark.ml/)
        - spark.mllib가 RDD 기반이고 spark.ml은 데이터프레임 기반
        - spark.mllib는 RDD위에서 동작하는 이전 라이브러리로 더 이상 업데이트가 안됨
- 항상 spark.ml을 사용할 것!
    - import [pyspark.ml](http://pyspark.ml/) (~~import pyspark.mllib~~)
- Spark ML의 장점
    - 원스톱 ML 프레임웍!
        - 데이터프레임과 SparkSQL등을 이용해 전처리
        - Spark ML을 이용해 모델 빌딩
        - ML Pipeline을 통해 모델 빌딩 자동화
        - MLflow로 모델 관리하고 서빙 (MLOps)
- 대용량 데이터도 처리 가능!

## 7. Spark 데이터 시스템 사용 예들

### 1. 기본적으로 대용량 데이터 배치 처리, 스트림 처리, 모델 빌딩

- 예 1) 대용량 비구조화된 데이터 처리하기 (ETL 혹은 ELT)
- 예 2) ML 모델에 사용되는 대용량 피쳐 처리 (배치/스트림)
- 예 3) Spark ML을 이용한 대용량 훈련 데이터 모델 학습

### 2. Spark 데이터 시스템 사용 예 1

- 대용량 **비구조화**된 데이터 처리하기 (Hive의 대체 기술)
- ETL 혹은 ELT

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/9bbc2a41-dc7b-47d9-8dd3-f46da3862f54)

### 3. Spark 데이터 시스템 사용 예 2

- ML 모델에 사용되는 대용량 피쳐 처리

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/dc7884a9-e603-4f64-835e-1b485e3f76f4)

# 7. Spark 프로그램 실행 옵션

## 1. Spark 프로그램 실경

### 1. 개발/테스트/학습 환경 (Interactive Clients)

- 노트북 (주피터, 제플린)
- Spark Shell

### 2. 프로덕션 환경 (Submit Job)

- spark-submit (command-line utility): 가장 많이 사용됨
- 데이터브릭스 노트북:
    - 노트북 코드를 주기적으로 실행해주는 것이 가능
- REST API:
    - Spark Standalone 모드에서만 가능
    - API를 통해 Spark 잡을 실행
    - 실행코드는 미리 HDFS등의 파일 시스템에 적재되어 있어야함
    

## 2. Spark 프로그램의 구조

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/6bea805e-d7ee-4ae9-8650-de5d1fcae8cb)

### 1. Driver

- 실행되는 코드의 마스터 역할 수행 (YARN의 Application Master)
- 사용자 코드를 실행하며 실행 모드(client, cluster)에 따라 실행되는 곳이 달라짐
- 코드를 실행하는데 필요한 리소스를 지정함
    - `-num-executors` : 전체 executor 개수
    - `--executor-cores` : 각 executor 당 core 개수
    - `--executor-memory` : 각 executor 당 memory 용
- SparkSession을 만들어 Spark 클러스터와 통신 수행
    - Cluster Manager (YARN의 경우 Resource Manager)
    - Executor (YARN의 경우 Container)
- 사용자 코드를 실제 Spark 태스크로 변환해 Spark 클러스터에서 실행

### 2. Executor

- 실제 태스크를 실행해주는 역할 수행 (JVM): Transformations, Actions
- YARN에서는 Container가 됨

## 3. Spark 클러스터 매니저 옵션

### 1. local[n]

- 개발/테스트용
    - Spark Shell, IDE, 노트북
- 하나의 JVM이 클러스터로 동작
    - Driver와 하나의 Executor 실행
- n은 코어의 수
    - Executor의 스레드 수가 됨
- local[*]는 무엇일까?
    - 컴퓨터에 있는 모든 코어 사용

- IF local[3]가 사용되면

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/112af834-b670-4848-b7e3-d67e1b0cdb2c)

-> 하나의 JVM 안에 3개의 Executor가 만들어짐 (4개의 쓰레드)


### 2. YARN

- 두 개의 실행 모드가 존재: Client vs. Cluster
1. Client 모드: Driver가 Spark 클러스터 밖에서 동작
  - YARN 기반 Spark 클러스터를 바탕으로 개발/테스트 등을 할 때 사용
2. Cluster 모드: Driver가 Spark 클러스터 안에서 동작
  - 하나의 Container 슬롯을 차지
  - 실제 프로덕션 운영에 사용되는 모드

![image](https://github.com/mini0-0/mini0-0.github.io/assets/63296983/382a1d75-ead2-4210-b144-76a14e6e90ca)

## 3. 기타

- Kubernetes
- Mesos
- Standalone

## 4. Spark 클러스터 매니저와 실행 모델 요약

| 클러스터 매니저 
 | 실행 모드 (deployed
mode) | 프로그램 실행 방식 |
| --- | --- | --- |
| local[n] | Client | Spark Shell, IDE, 노트북
YARN Client |
| YARN | Client | Spark Shell, 노트북 |
| YARN | Cluster | spark-submit |


